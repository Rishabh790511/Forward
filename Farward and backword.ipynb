{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad16a35-b926-42bf-b17d-22f56d565f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1. What is the purpose of forward propagation in a neural network?\n",
    "\n",
    "Forward propagation is the process of computing the output of a neural network given an input. The purpose of forward propagation is to propagate the input data through the network's layers, applying weights and biases, and activating neurons using activation functions, ultimately producing an output prediction.\n",
    "\n",
    "Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\n",
    "\n",
    "In a single-layer feedforward neural network, forward propagation involves the following steps:\n",
    "\n",
    "Compute the weighted sum of inputs:\n",
    "�\n",
    "=\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "×\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "z=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " w \n",
    "i\n",
    "​\n",
    " ×x \n",
    "i\n",
    "​\n",
    " +b\n",
    "where \n",
    "�\n",
    "�\n",
    "w \n",
    "i\n",
    "​\n",
    "  are the weights, \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  are the inputs, \n",
    "�\n",
    "b is the bias, and \n",
    "�\n",
    "n is the number of inputs.\n",
    "\n",
    "Apply the activation function:\n",
    "�\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "a=f(z)\n",
    "where \n",
    "�\n",
    "f is the activation function and \n",
    "�\n",
    "a is the output of the neuron.\n",
    "\n",
    "Q3. How are activation functions used during forward propagation?\n",
    "\n",
    "Activation functions are applied to the weighted sum of inputs during forward propagation to introduce non-linearity into the neural network. They help the network learn complex patterns in the data and control the output range of neurons.\n",
    "\n",
    "Q4. What is the role of weights and biases in forward propagation?\n",
    "\n",
    "Weights and biases are parameters in neural networks that are adjusted during training to minimize the difference between the predicted output and the actual output. During forward propagation, weights are multiplied by inputs, and biases are added to the result, influencing the output of neurons.\n",
    "\n",
    "Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?\n",
    "\n",
    "The softmax function is used in the output layer during forward propagation for multi-class classification tasks. It converts raw scores (logits) into probabilities, ensuring that the output values sum up to 1. This allows the network to predict the probability of each class being the correct label.\n",
    "\n",
    "Q6. What is the purpose of backward propagation in a neural network?\n",
    "\n",
    "Backward propagation, also known as backpropagation, is the process of computing gradients of the loss function with respect to the weights and biases of the network. Its purpose is to update the weights and biases in the direction that minimizes the loss, thereby optimizing the network's performance.\n",
    "\n",
    "Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?\n",
    "\n",
    "In a single-layer feedforward neural network, backward propagation involves calculating the gradients of the loss function with respect to the weights and biases using techniques such as the chain rule and gradient descent.\n",
    "\n",
    "Q8. Can you explain the concept of the chain rule and its application in backward propagation?\n",
    "\n",
    "The chain rule is a fundamental concept in calculus that allows for the computation of the derivative of a composite function. In the context of neural networks, the chain rule is used during backward propagation to calculate the gradients of the loss function with respect to the parameters of the network. It breaks down the overall derivative into smaller derivatives, which are then multiplied together to obtain the final gradient.\n",
    "\n",
    "Q9. What are some common challenges or issues that can occur during backward propagation, and how can they be addressed?\n",
    "\n",
    "Some common challenges during backward propagation include vanishing gradients, exploding gradients, and numerical instability. These issues can be addressed by using techniques such as gradient clipping, weight initialization methods, and using appropriate activation functions like ReLU or its variants. Additionally, using techniques like batch normalization and residual connections can also help stabilize training during backward propagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
